name: Deploy Open WebUI with DeepSeek Models

on:
  push:
    branches: [ main ]

jobs:
  deploy:
    runs-on: ubuntu-latest

    steps:
      # 1. Checkout the repository code.
      - name: Checkout code
        uses: actions/checkout@v3

      # 2. Set up Docker Buildx (optional but recommended for caching and advanced builds).
      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v2

      # 3. Pull the official Open WebUI (Ollama) image and retag it.
      - name: Pull Open WebUI (Ollama) image
        run: |
          docker pull ghcr.io/open-webui/open-webui:ollama
          docker tag ghcr.io/open-webui/open-webui:ollama my-openwebui:latest

      # 4. Run the Open WebUI container using host networking.
      #    This makes sure that the internal Ollama API (default 127.0.0.1:11434) is reachable.
      - name: Run Open WebUI container with host networking
        run: |
          docker run -d --network=host \
            -e OLLAMA_BASE_URL="http://127.0.0.1:11434" \
            -e OLLAMA_MODELS="/models" \
            --name openwebui \
            --restart always \
            my-openwebui:latest

      # 5. Wait for the Ollama API to become responsive.
      - name: Wait for Ollama API to be ready
        run: |
          echo "Waiting for Ollama API to be ready..."
          for i in {1..12}; do
            if docker exec openwebui curl -s http://127.0.0.1:11434/health | grep -qi "ok"; then
              echo "Ollama API is ready."
              exit 0
            fi
            echo "Not ready yet, sleeping 5 seconds..."
            sleep 5
          done
          echo "Ollama API did not become ready in time."
          exit 1

      # 6. Download the DeepSeek models.
      - name: Download DeepSeek Models
        run: |
          MODELS=(
            "deepseek-v2.5:236"
            "deepseek-r1:8b"
            "deepseek-r1:14b"
            "deepseek-r1:32b"
            "deepseek-r1:70b"
            "deepseek-r1:671b"
            "deepseek-llm:7b"
            "deepseek-llm:67b"
          )
          for model in "${MODELS[@]}"; do
            echo "Pulling model: $model"
            docker exec openwebui ollama pull "$model" || echo "$model model manifest not found or API error"
          done

      # 7. Launch each DeepSeek model with a test prompt.
      - name: Launch DeepSeek Models (Test Run)
        run: |
          MODELS=(
            "deepseek-v2.5:236"
            "deepseek-r1:8b"
            "deepseek-r1:14b"
            "deepseek-r1:32b"
            "deepseek-r1:70b"
            "deepseek-r1:671b"
            "deepseek-llm:7b"
            "deepseek-llm:67b"
          )
          for model in "${MODELS[@]}"; do
            echo "Launching model: $model"
            docker exec openwebui ollama run "$model" "Hello from $model" || echo "Failed to launch $model"
          done

      # 8. Check that Open WebUI is responding on the expected port.
      - name: Check Open WebUI response
        run: |
          sleep 10
          if curl -s http://localhost:8080/auth | grep -q "Sign Up"; then
            echo "Open WebUI is running."
          else
            echo "Error: Open WebUI did not respond as expected."
            exit 1
          fi
