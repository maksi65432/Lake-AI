name: Deploy Open WebUI with DeepSeek Models

on:
  push:
    branches: [ main ]

jobs:
  deploy:
    runs-on: ubuntu-latest

    steps:
      # 1. Checkout the repository code.
      - name: Checkout code
        uses: actions/checkout@v3

      # 2. Pull the official Open WebUI (Ollama) image and retag it locally.
      - name: Pull Open WebUI (Ollama) image
        run: |
          docker pull ghcr.io/open-webui/open-webui:ollama
          docker tag ghcr.io/open-webui/open-webui:ollama my-openwebui:latest

      # 3. Run the Open WebUI container with host networking and necessary environment variables.
      - name: Run Open WebUI container
        run: |
          docker run -d --network=host \
            -e OLLAMA_BASE_URL="http://127.0.0.1:11434" \
            -e OLLAMA_MODELS="/models" \
            --name openwebui \
            --restart always \
            my-openwebui:latest

      # 4. Wait for the container to initialize.
      - name: Wait for container startup
        run: |
          echo "Waiting 20 seconds for container startup..."
          sleep 20

      # 5. Ensure the Ollama service is running inside the container.
      - name: Ensure Ollama service is running
        run: |
          echo "Checking if Ollama service is running..."
          docker exec openwebui sh -c 'if ! pgrep -f "ollama serve" > /dev/null; then echo "Starting Ollama service..."; ollama serve & else echo "Ollama service already running"; fi'
          # Allow time for the service to start
          sleep 10

      # 6. Wait until the Ollama API is responsive.
      - name: Wait for Ollama API to be ready
        run: |
          echo "Waiting for Ollama API to be ready..."
          for i in {1..12}; do
            if docker exec openwebui curl -s http://127.0.0.1:11434/health | grep -qi "ok"; then
              echo "Ollama API is ready."
              exit 0
            fi
            echo "Ollama API not ready yet; sleeping 5 seconds..."
            sleep 5
          done
          echo "Ollama API did not become ready in time."
          exit 1

      # 7. Download the DeepSeek models via Ollama.
      - name: Download DeepSeek Models
        run: |
          MODELS=(
            "deepseek-v2.5:236"
            "deepseek-r1:8b"
            "deepseek-r1:14b"
            "deepseek-r1:32b"
            "deepseek-r1:70b"
            "deepseek-r1:671b"
            "deepseek-llm:7b"
            "deepseek-llm:67b"
          )
          for model in "${MODELS[@]}"; do
            echo "Pulling model: $model"
            docker exec openwebui ollama pull "$model" || echo "$model model manifest not found or API error"
          done

      # 8. Launch each DeepSeek model with a test prompt.
      - name: Launch DeepSeek Models (Test Run)
        run: |
          MODELS=(
            "deepseek-v2.5:236"
            "deepseek-r1:8b"
            "deepseek-r1:14b"
            "deepseek-r1:32b"
            "deepseek-r1:70b"
            "deepseek-r1:671b"
            "deepseek-llm:7b"
            "deepseek-llm:67b"
          )
          for model in "${MODELS[@]}"; do
            echo "Launching model: $model"
            docker exec openwebui ollama run "$model" "Hello from $model" || echo "Failed to launch $model"
          done

      # 9. Check that Open WebUI is responding.
      - name: Check Open WebUI response
        run: |
          sleep 10
          if curl -s http://localhost:8080/auth | grep -q "Sign Up"; then
            echo "Open WebUI is running."
          else
            echo "Error: Open WebUI did not respond as expected."
            exit 1
          fi
